# -*- coding: utf-8 -*-
"""Prediction of Loan Approval.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wVtVmKrUtLccg2ppqu84EgHiHuQDsmIA

# Machine Learning Project: Predicting Loan Approval

## Full Analysis of ML Projects

  In order to predict the loan approval status based on different applicant characteristics, this project examines the loan_data dataset.

## Table of Contents
1. [Introduction](#introduction)
2. [Dataset Overview](#dataset-overview)
3. [Exploratory Data Analysis](#eda)
4. [Data Preprocessing](#preprocessing)
5. [Model Building](#model-building)
6. [Model Evaluation](#evaluation)
7. [Insights & Conclusions](#insights-conclusions)
8. [Summary](#summary)

## Introduction

This machine learning project is about predicting whether a loan will be approved or not, based on different factors related to the person applying for the loan.
The dataset includes personal and financial details about loan applicants, which helps us find out which factors are most important in deciding whether a loan is approved.

### Objectives:
- Look at the loan data to see what patterns exist in loan approvals
- Create models that can predict the chances of a loan being approved
- Find out which factors have the biggest impact on loan approval decisions
- Check how well the models work and give useful information that can be used to make better decisions
"""

# from google.colab import drive
# drive.mount('/content/gdrive')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# Set style for plots
plt.style.use('seaborn-v0_8')
sns.set_palette("viridis") # Changed from "husl" to "viridis"
pd.set_option('display.max_columns', None)

"""## Dataset Overview <a id='dataset-overview'></a>

Let's start by loading the dataset and checking how it's structured, its dimension, and some basic facts about the data.
"""

# df = pd.read_csv('/content/gdrive/MyDrive/DataSets/SMIT Assignment Loan_Approval_Prediction(dataset)/loan_data.csv')
df = pd.read_csv(r"C:\Zukhruf\Github\Assignments\4th\Dataset\loan_data.csv")

print("Dataset Shape:\n", df.shape)

print("\nDataset Info:\n", df.info())

print("\nFirst 5 rows:\n", df.head())

print("\nMissing Values:\n", df.isnull().sum(),"\n")

print("Names of coloums:")
for col in df.columns:
    print(f"- {col}")

df.describe().T

"""## Exploratory Data Analysis (EDA) <a id='eda'></a>

Let's insight the relationships between different features and the targeted variable.
"""
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler

# Identify numeric and text-based features
cat_features = df.select_dtypes(include=['object']).columns.tolist()
num_features = df.select_dtypes(include=[np.number]).columns.tolist()

print(f"Categorical features found: {cat_features}")
print(f"Numerical features found: {num_features}")

# Analyze null entries in the dataset
null_counts = df.isnull().sum()

if null_counts.values.sum() == 0:
    print("There is no null value")
else:
    print("Null values per coloums")
    print(null_counts[null_counts != 0])

# Check target variable distribution
target_dist = df['loan_status'].value_counts()

print("Target Variable Distribution:")
print(target_dist)

neg_pct = (target_dist[0] / len(df)) * 100
pos_pct = (target_dist[1] / len(df)) * 100

print(f"\nClass Imbalance Analysis:")
print(f"Rejected (0): {target_dist[0]} samples ({neg_pct:.2f}%)")
print(f"Approved (1): {target_dist[1]} samples ({pos_pct:.2f}%)")

plt.figure(figsize=(12, 5))

# Pie Chart
plt.subplot(1, 2, 1)
plt.pie(
    target_dist.values,
    labels=target_dist.index.map({0: 'Rejected', 1: 'Approved'}),
    autopct='%1.1f%%',
    startangle=90
)
plt.title('Loan Status Distribution')

# Count Plot
plt.subplot(1, 2, 2)
sns.countplot(data=df, x='loan_status')
plt.title('Loan Status Count')
plt.xlabel('Loan Status (0 = Rejected, 1 = Approved)')
plt.ylabel('Count')

plt.tight_layout()
plt.show()

# Visualize distributions of numeric attributes
fig, plot_axes = plt.subplots(3, 3, figsize=(18, 14))
flat_axes = plot_axes.flatten()

for idx, feature in enumerate(num_features):
    if idx < len(flat_axes):
        flat_axes[idx].hist(
            df[feature],
            bins=28,
            edgecolor='black',
            alpha=0.75
        )
        flat_axes[idx].set_title(f'Feature Distribution: {feature}')
        flat_axes[idx].set_xlabel(feature)
        flat_axes[idx].set_ylabel('Count')

# Disable extra empty plots
for k in range(idx + 1, len(flat_axes)):
    flat_axes[k].axis('off')

plt.tight_layout()
plt.show()

# Visual distribution of categorical variables
fig, grid_axes = plt.subplots(2, 3, figsize=(20, 12))
flat_axes = grid_axes.flatten()

for idx, category in enumerate(cat_features):
    if idx < len(flat_axes):
        sns.countplot(
            data=df,
            x=category,
            ax=flat_axes[idx]
        )
        flat_axes[idx].set_title(f'Category Distribution: {category}')
        flat_axes[idx].tick_params(axis='x', rotation=40)

# Disable remaining empty plots
for extra in range(idx + 1, len(flat_axes)):
    flat_axes[extra].axis('off')

plt.tight_layout()
plt.show()

# Boxplot comparison of numerical features against loan status
fig, axis_grid = plt.subplots(3, 3, figsize=(18, 14))
flat_axes = axis_grid.flatten()

plot_index = 0
for feature in num_features:
    if feature != 'loan_status' and plot_index < len(flat_axes):
        sns.boxplot(
            data=df,
            x='loan_status',
            y=feature,
            ax=flat_axes[plot_index]
        )
        flat_axes[plot_index].set_title(f'{feature} vs Loan Status')
        plot_index += 1

# Turn off unused axes
for remaining in range(plot_index, len(flat_axes)):
    flat_axes[remaining].axis('off')

plt.tight_layout()
plt.show()

# Heatmap of correlations among numeric features
plt.figure(figsize=(12, 10))
num_corr = df[num_features].corr()
sns.heatmap(
    num_corr,
    annot=True,
    cmap='coolwarm',
    center=0,
    square=True,
    linewidths=0.5
)
plt.title('Correlation Matrix of Numerical Features')
plt.show()

# Identify and display highly correlated feature pairs
strong_corr_pairs = []
for row_idx in range(len(num_corr.columns)):
    for col_idx in range(row_idx + 1, len(num_corr.columns)):
        corr_value = num_corr.iloc[row_idx, col_idx]
        if abs(corr_value) > 0.5:
            strong_corr_pairs.append((
                num_corr.columns[row_idx],
                num_corr.columns[col_idx],
                corr_value
            ))

if strong_corr_pairs:
    print("Highly correlated feature pairs (|correlation| > 0.5):")
    for f1, f2, val in strong_corr_pairs:
        print(f"{f1} - {f2}: {val:.3f}")
else:
    print("No highly correlated feature pairs detected (|correlation| > 0.5)")

# Normalized stacked bar charts for categorical features against loan status
fig, cat_axes = plt.subplots(2, 3, figsize=(20, 12))
flat_axes = cat_axes.flatten()

for idx, category in enumerate(cat_features):
    if idx < len(flat_axes):
        # Create normalized crosstab for proportion visualization
        prop_table = pd.crosstab(df[category], df['loan_status'], normalize='index')
        prop_table.plot(
            kind='bar',
            stacked=True,
            ax=flat_axes[idx],
            color=['#FF6F61', '#6B5B95']
        )
        flat_axes[idx].set_title(f'{category} vs Loan Status (Proportion)')
        flat_axes[idx].set_xlabel(category)
        flat_axes[idx].set_ylabel('Proportion')
        flat_axes[idx].legend(title='Loan Status', labels=['Rejected', 'Approved'])
        flat_axes[idx].tick_params(axis='x', rotation=40)

# Turn off unused axes
for extra in range(idx + 1, len(flat_axes)):
    flat_axes[extra].axis('off')

plt.tight_layout()
plt.show()

"""## Data Preprocessing <a id='preprocessing'></a>

Next, we'll get the data ready for use in machine learning by converting text-based categories into numbers and adjusting the numerical values to a standard range.

"""

# Encode categorical features using LabelEncoder
label_encoders = {}
df_encoded = df.copy()

for feature in cat_features:
    encoder = LabelEncoder()
    df_encoded[feature] = encoder.fit_transform(df_encoded[feature])
    label_encoders[feature] = encoder

print("Encoded categorical features:")
for feature in cat_features:
    mapping = dict(zip(
        label_encoders[feature].classes_,
        label_encoders[feature].transform(label_encoders[feature].classes_)
    ))
    print(f"{feature}: {mapping}")

# Separate features and target variable
X_features = df_encoded.drop('loan_status', axis=1)
y_target = df_encoded['loan_status']

print(f"Features shape: {X_features.shape}")
print(f"Target shape: {y_target.shape}")
print(f"Feature columns: {X_features.columns.tolist()}")

# Split dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
    X_features,
    y_target,
    test_size=0.2,
    random_state=42,
    stratify=y_target
)

print(f"Training set: {X_train.shape[0]} samples ({X_train.shape[0]/len(X_features)*100:.1f}%)")
print(f"Test set: {X_test.shape[0]} samples ({X_test.shape[0]/len(X_features)*100:.1f}%)")
print("Training target distribution:")
print(y_train.value_counts().sort_index())
print("Test target distribution:")
print(y_test.value_counts().sort_index())

# Apply standard scaling to features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("Standard scaling of features completed successfully")

"""## Model Building <a id='model-building'></a>

Now, we'll create and train several machine learning models to predict whether a loan will be approved.
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression

from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve

# Initialize machine learning models
ml_models = {
    'Logistic Regression': LogisticRegression(random_state=42),
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
    'SVM': SVC(probability=True, random_state=42)
}

# Train and evaluate each model
evaluation_results = {}

for model_name, clf in ml_models.items():
    print(f"Training {model_name}...")

    # Use scaled features for Logistic Regression and SVM, original features for Random Forest
    if model_name in ['Logistic Regression', 'SVM']:
        clf.fit(X_train_scaled, y_train)
        y_pred = clf.predict(X_test_scaled)
        y_pred_proba = clf.predict_proba(X_test_scaled)[:, 1]
    else:
        clf.fit(X_train, y_train)
        y_pred = clf.predict(X_test)
        y_pred_proba = clf.predict_proba(X_test)[:, 1]

    # Calculate evaluation metrics
    acc = accuracy_score(y_test, y_pred)
    auc = roc_auc_score(y_test, y_pred_proba)

    evaluation_results[model_name] = {
        'model': clf,
        'accuracy': acc,
        'auc_score': auc,
        'predictions': y_pred,
        'probabilities': y_pred_proba
    }

    print(f"{model_name} - Accuracy: {acc:.4f}, AUC: {auc:.4f}\n")

# Compare performance metrics of trained models
model_perf_df = pd.DataFrame({
    'Model': list(evaluation_results.keys()),
    'Accuracy': [evaluation_results[model]['accuracy'] for model in evaluation_results.keys()],
    'AUC Score': [evaluation_results[model]['auc_score'] for model in evaluation_results.keys()]
})

print("Performance comparison of models:")
print(model_perf_df)

# Visualize model comparison
fig, comp_axes = plt.subplots(1, 2, figsize=(15, 5))

# Accuracy comparison
comp_axes[0].bar(model_perf_df['Model'], model_perf_df['Accuracy'], color='#4C72B0')
comp_axes[0].set_title('Model Accuracy Comparison')
comp_axes[0].set_ylabel('Accuracy')
comp_axes[0].tick_params(axis='x', rotation=40)

# AUC score comparison
comp_axes[1].bar(model_perf_df['Model'], model_perf_df['AUC Score'], color='#55A868')
comp_axes[1].set_title('Model AUC Score Comparison')
comp_axes[1].set_ylabel('AUC Score')
comp_axes[1].tick_params(axis='x', rotation=40)

plt.tight_layout()
plt.show()

"""## Model Evaluation <a id='evaluation'></a>

After that, we'll assess the top-performing model using detailed performance measures and visual charts.
"""

# Select the top-performing model based on AUC score
best_model_name = max(evaluation_results, key=lambda name: evaluation_results[name]['auc_score'])
best_model = evaluation_results[best_model_name]['model']
best_preds = evaluation_results[best_model_name]['predictions']
best_probs = evaluation_results[best_model_name]['probabilities']

print(f"Top Performing Model: {best_model_name}")
print(f"AUC Score: {evaluation_results[best_model_name]['auc_score']:.4f}")
print(f"Accuracy: {evaluation_results[best_model_name]['accuracy']:.4f}")

# Display detailed classification report for the top model
print(f"Classification Report for {best_model_name}:\n")
print(classification_report(
    y_test,
    best_preds,
    target_names=['Rejected', 'Approved']
))

# Plot confusion matrix for the top-performing model
plt.figure(figsize=(8, 6))
cm = confusion_matrix(y_test, best_preds)

sns.heatmap(
    cm,
    annot=True,
    fmt='d',
    xticklabels=['Rejected', 'Approved'],
    yticklabels=['Rejected', 'Approved']
)
plt.title(f'Confusion Matrix - {best_model_name}')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

# Plot ROC curve for the top-performing model
plt.figure(figsize=(8, 6))
fpr, tpr, _ = roc_curve(y_test, best_probs)
auc_score = roc_auc_score(y_test, best_probs)

plt.plot(
    fpr, tpr,
    lw=2,
    label=f'ROC Curve (AUC = {auc_score:.4f})'
)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title(f'ROC Curve - {best_model_name}')
plt.legend(loc="lower right")
plt.grid(True)
plt.show()

# Display feature importance for Random Forest model
if best_model_name == 'Random Forest':
    feature_importance_df = pd.DataFrame({
        'Feature': X_features.columns,
        'Importance': best_model.feature_importances_
    }).sort_values(by='Importance', ascending=False)

    print("Top 10 Important Features:")
    print(feature_importance_df.head(10))

    # Plot top 10 feature importances
    plt.figure(figsize=(10, 8))
    sns.barplot(
        data=feature_importance_df.head(10),
        x='Importance',
        y='Feature',
        palette='viridis'
    )
    plt.title(f'Top 10 Feature Importances - {best_model_name}')
    plt.xlabel('Importance')
    plt.ylabel('Feature')
    plt.tight_layout()
    plt.show()
else:
    print(f"Feature importance is only available for tree-based models. {best_model_name} does not provide it directly.")

"""## Insights & Conclusions <a id='insights-conclusions'></a>

Finally, we'll review the main results from our analysis and how well the model performed.
"""

# Summary of key insights from the analysis
print("=== KEY INSIGHTS FROM THE LOAN APPROVAL ANALYSIS ===\n")

# 1. Dataset Overview
print("1. DATASET OVERVIEW:")
print(f"   - Total records: {len(df)}")
print(f"   - Total features (excluding target): {len(df.columns) - 1}")
print(f"   - Target variable distribution: {target_dist[0]} rejected, {target_dist[1]} approved")
print(f"   - Class imbalance ratio: {(target_dist[0]/target_dist[1]):.2f}:1 (rejected:approved)")

# 2. Data Quality
print("\n2. DATA QUALITY:")
if null_counts.sum() == 0:
    print("   - No missing values detected in the dataset")
else:
    print(f"   - Total missing values: {null_counts.sum()}")

# 3. Feature Characteristics
print("\n3. FEATURE CHARACTERISTICS:")
print(f"   - Numerical features (excluding target): {len(num_features) - 1}")
print(f"   - Categorical features: {len(cat_features)}")

# 4. Model Performance
print("\n4. MODEL PERFORMANCE:")
print(f"   - Best performing model: {best_model_name}")
print(f"   - Accuracy: {evaluation_results[best_model_name]['accuracy']:.4f}")
print(f"   - AUC Score: {evaluation_results[best_model_name]['auc_score']:.4f}")

# 5. Business Implications
print("\n5. BUSINESS IMPLICATIONS:")
print("   - The model can assist in automating loan approval decisions")
print("   - High AUC score indicates strong discriminative ability")
print("   - Feature importance highlights key factors affecting loan approvals")

if best_model_name == 'Random Forest':
    print(f"   - Most important factor: {feature_importance_df.iloc[0]['Feature']} "
          f"(importance: {feature_importance_df.iloc[0]['Importance']:.4f})")

"""# **Conclusion**

## Summary <a id='summary'></a>

This machine learning project created a model that can predict whether a loan will be approved based on the applicant's details. The data used had 45,000 entries and included various types of information about the applicants, their financial situation, and the loans they applied for.

### Key Achievements:


1.   We looked closely at the data to understand what it contained.
2.   We found out how different applicant traits relate to whether their loan gets approved.
3.   We built and tested three different machine learning models.
4.   The models performed well, with high accuracy and AUC scores, showing they can make reliable predictions.
5.   We figured out which features are most important when deciding on loan approvals.



### Recommendations:

1.   Use the best model to help make loan approval decisions.
2.   Keep checking how well the model works and update it when necessary.
3.   Think about adding more features that could help the model perform better.
4.   Make sure the model is fair and doesnâ€™t treat certain groups unfairly.

## Overall

This project shows how useful machine learning can be in making financial decisions, and it sets a good starting point for creating automated loan approval systems.
"""

