# -*- coding: utf-8 -*-
"""Prediction of House Price.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LkDTO7ZeaLVoFU7uEJxzWgO3cZFRRUGd

# **Prediction of House Price**

## Objective
This project's goal is to estimate house prices based on many property-related characteristics, including size, number of rooms, location, and general condition. Data exploration, fundamental analysis, preprocessing, model training, and performance evaluation are all part of the project.


## Information about the Dataset
The dataset includes about 2000 house records with the following characteristics:


- **House_ID**: Every property has an unique ID.

- **Area_sqft**: The house's total covered area in square feet

- **No_of_Bedrooms**: The total number of bedrooms in the home

- **No_of_Bathrooms**: Total number of bathrooms

- **Total_Floors**: The number of floors in the building

- **Construction_Year**: The year the house was built

- **Location_Type**: Location category (rural, urban, suburban, downtown)

- **House_Condition**: The house's physical condition (excellent, good, fair, or poor).

- **Garage_Available**: Indicates if a garage is available (Yes/No).

- **House_Price**: "Target variable" that represents the price of house.
"""

# Mounting Drive
# from google.colab import drive
# drive.mount('/content/drive')

"""## Importing The Libraries"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# Set style for plots
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

"""## Loading Dataset & Exploring The Data"""

# df = pd.read_csv('/content/drive/MyDrive/DataSets/SMIT Assignment House_Price_Prediction_System(Dataset)/House Price Prediction Dataset.csv')
df= pd.read_csv(r"c:\Zukhruf\Github\Assignments\3rd\Dataset\House Price Prediction Dataset.csv")

print("Dataset Shape:\n", df.shape)

print("\nDataset Info:\n", df.info())

print("\nFirst 5 rows:\n", df.head())

print("Basic Statistics:\n", df.describe())

print("\nMissing Values:\n", df.isnull().sum())

"""## Exploratory Data Analysis (EDA)

### 1. Analysis Of House Prices
"""

plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 2)
plt.boxplot(df["Price"])
plt.title("House Price Box Plot")
plt.ylabel("House Price")

plt.tight_layout()
plt.show()

plt.figure(figsize=(18, 8))

plt.subplot(1, 2, 1)
plt.hist(df["Price"], bins=50, edgecolor="black")
plt.title("House Price Distribution")
plt.xlabel("House Price")
plt.ylabel("Frequency")

"""### 2. Analysis Of Categorical Features"""

cat_cols = ["Location", "Condition", "Garage"]

fig, axes = plt.subplots(1, 3, figsize=(18, 5))

for i, col in enumerate(cat_cols):
    counts = df[col].value_counts()

    axes[i].bar(
        counts.index,
        counts.values,
        color=["#1f77b4", "#6baed6", "#9ecae1", "#c6dbef"]

    )

    axes[i].set_title(f"{col} Distribution")
    axes[i].set_xlabel(col)
    axes[i].set_ylabel("Count")
    axes[i].tick_params(axis="x", rotation=45)

plt.tight_layout()
plt.show()

"""### 3. Analysis Of Numerical Features"""

num_features = ["Area", "Bedrooms", "Bathrooms", "Floors", "YearBuilt"]

fig, axes = plt.subplots(2, 3, figsize=(15, 10))
axes = axes.flatten()

for i, col in enumerate(num_features):
    axes[i].hist(df[col], bins=30, color="lightblue",edgecolor="black")
    axes[i].set_title(f"{col} Distribution")
    axes[i].set_xlabel(col)
    axes[i].set_ylabel("Frequency")

# Removing  unused subplot
fig.delaxes(axes[-1])

plt.tight_layout()
plt.show()

"""### 4. Correlation Matrix Of Numerical Features"""

plt.figure(figsize=(10, 8))

num_corr = df.select_dtypes(include=[np.number]).corr()

sns.heatmap(
    num_corr,
    annot=True,
    cmap='coolwarm',   # coolwarm → Blues (more academic & clean)
    center=0,
    square=True,
    fmt=".2f",
    cbar_kws={"shrink": 0.8}
)

plt.title("Numerical Features Correlation Matrix")
plt.tight_layout()
plt.show()

"""### 5. Relationship Between Numerical Features And House Price

"""

fig, axes = plt.subplots(2, 3, figsize=(18, 10))
axes = axes.flatten()

for i, feature in enumerate(num_features):
    axes[i].scatter(
        df[feature],
        df["Price"],
        alpha=0.6,
        color="steelblue"
    )

    axes[i].set_xlabel(feature)
    axes[i].set_ylabel("Price")
    axes[i].set_title(f"{feature} vs Price")

    # Trend line
    coeff = np.polyfit(df[feature], df["Price"], 1)
    trend_fn = np.poly1d(coeff)
    axes[i].plot(
        df[feature],
        trend_fn(df[feature]),
        linestyle="--",
        color="darkred",
        alpha=0.8
    )

# Remove unused subplot
fig.delaxes(axes[-1])

plt.tight_layout()
plt.show()

"""### 6. Price Comparison Across Categorical Features

"""

fig, axes = plt.subplots(1, 3, figsize=(18, 5))

for i, feature in enumerate(cat_cols):
    df.boxplot(
        column="Price",
        by=feature,
        ax=axes[i],
        patch_artist=True,
        boxprops=dict(facecolor="#1f77b4")
    )
    axes[i].set_title(f"{feature} vs Price")
    axes[i].set_xlabel(feature)
    axes[i].set_ylabel("Price")

# Remove automatic pandas title
plt.suptitle("")
plt.tight_layout()
plt.show()

"""## Data Pre-processing"""

from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Encode categorical columns
le_location = LabelEncoder()
le_condition = LabelEncoder()
le_garage = LabelEncoder()

df_proc = df.copy()
df_proc["Location_enc"] = le_location.fit_transform(df["Location"])
df_proc["Condition_enc"] = le_condition.fit_transform(df["Condition"])
df_proc["Garage_enc"] = le_garage.fit_transform(df["Garage"])

# Define features and target (shuffled order)
feature_cols = [
    "Bedrooms",
    "Floors",
    "Bathrooms",
    "Garage_enc",
    "Location_enc",
    "Area",
    "YearBuilt",
    "Condition_enc"
]


X = df_proc[feature_cols]
y = df_proc["Price"]

print("Features shape:", X.shape)
print("Target shape:", y.shape)
print("\nFeature columns:", X.columns.tolist())

# Split dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=40
)

print("Training shape:", X_train.shape)
print("Testing shape:", X_test.shape)

# Feature scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("\nFeature scaling done successfully")

"""## Model Training & Evaluation"""

from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# Defining models to train
model_dict = {
    "Linear Regression": LinearRegression(),
    "Decision Tree": DecisionTreeRegressor(random_state=42),
    "Random Forest": RandomForestRegressor(n_estimators=100, random_state=42)
}

model_results = {}

for model_name, mdl in model_dict.items():
    # Train model
    if model_name == "Linear Regression":
        mdl.fit(X_train_scaled, y_train)
        predictions = mdl.predict(X_test_scaled)
    else:
        mdl.fit(X_train, y_train)
        predictions = mdl.predict(X_test)

    mse = mean_squared_error(y_test, predictions)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_test, predictions)
    r2 = r2_score(y_test, predictions)

    # Store results
    model_results[model_name] = {
        "MSE": mse,
        "RMSE": rmse,
        "MAE": mae,
        "R2 Score": r2,
        "Predictions": predictions
    }

    # Print summary
    print(f"{model_name} Performance:")
    print(f"  MSE   : {mse:.2f}")
    print(f"  RMSE  : {rmse:.2f}")
    print(f"  MAE   : {mae:.2f}")
    print(f"  R2    : {r2:.4f}")
    print()

# Compare model performances
model_names = list(model_results.keys())
r2_scores = [model_results[m]['R2 Score'] for m in model_names]
rmse_values = [model_results[m]['RMSE'] for m in model_names]

fig, axes = plt.subplots(1, 2, figsize=(15, 5))

# R² Score comparison
bars_r2 = axes[0].bar(model_names, r2_scores, color="#1f77b4")
axes[0].set_title("Model Comparison - R² Score")
axes[0].set_ylabel("R² Score")
axes[0].set_ylim(0, 1)

for bar, score in zip(bars_r2, r2_scores):
    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                 f"{score:.3f}", ha="center", va="bottom")

# RMSE comparison
bars_rmse = axes[1].bar(model_names, rmse_values, color="#ff7f0e")
axes[1].set_title("Model Comparison - RMSE")
axes[1].set_ylabel("RMSE")

for bar, rmse in zip(bars_rmse, rmse_values):
    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(rmse_values)*0.01,
                 f"{rmse:.0f}", ha="center", va="bottom")

plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# Feature importance from Random Forest
rf_model = model_dict['Random Forest']
feature_importance = rf_model.feature_importances_
feature_names = X.columns

importance_df = pd.DataFrame({
    "Feature": feature_names,
    "Importance": feature_importance
}).sort_values(by="Importance", ascending=False)

print("Random Forest Feature Importance:")
print(importance_df)

plt.figure(figsize=(10, 6))
sns.barplot(
    data=importance_df,
    x="Importance",
    y="Feature",
    palette='viridis'

)
plt.title("Feature Importance for House Price Prediction")
plt.xlabel("Importance Score")
plt.ylabel("")
plt.tight_layout()
plt.show()

# Actual vs Predicted plot for the best model
best_model_name = max(model_results, key=lambda x: model_results[x]['R2 Score'])
best_predictions = model_results[best_model_name]['Predictions']

plt.figure(figsize=(10, 6))
plt.scatter(
    y_test,
    best_predictions,
    alpha=0.6,
    color="#1f77b4"
)

plt.plot(
    [y_test.min(), y_test.max()],
    [y_test.min(), y_test.max()],
    'r--',
    lw=2
)

plt.xlabel("Actual Prices")
plt.ylabel("Predicted Prices")
plt.title(f"Actual vs Predicted Prices - {best_model_name}")

plt.text(
    0.05, 0.95,
    f"R² Score: {model_results[best_model_name]['R2 Score']:.4f}",
    transform=plt.gca().transAxes,
    verticalalignment='top',
    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5)
)

plt.tight_layout()
plt.show()

"""## Model Improvement - Hyperparameter Tuning for Random Forest"""

from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestRegressor

# Parameter grid for Random Forest
param_grid = {
    "n_estimators": [50, 100, 200],
    "max_depth": [None, 10, 20, 30],
    "min_samples_split": [2, 5, 10],
    "min_samples_leaf": [1, 2, 4]}

rf_base = RandomForestRegressor(random_state=42)

grid_rf = GridSearchCV(
    estimator=rf_base,
    param_grid=param_grid,
    cv=3,
    scoring="r2",
    n_jobs=-1,
    verbose=1
)

grid_rf.fit(X_train, y_train)


best_rf_model = grid_rf.best_estimator_
print("Best RF parameters:", grid_rf.best_params_)


y_pred_tuned = best_rf_model.predict(X_test)


mse_tuned = mean_squared_error(y_test, y_pred_tuned)
rmse_tuned = np.sqrt(mse_tuned)
mae_tuned = mean_absolute_error(y_test, y_pred_tuned)
r2_tuned = r2_score(y_test, y_pred_tuned)

print("\nTuned Random Forest Performance:")
print(f"  MSE  : {mse_tuned:.2f}")
print(f"  RMSE : {rmse_tuned:.2f}")
print(f"  MAE  : {mae_tuned:.2f}")
print(f"  R2   : {r2_tuned:.4f}")

"""## Insights and Key Findings"""

# Data overview
print("1. Dataset Summary:")
print(f"   - Total listings analyzed: {len(df)}")
print(f"   - Average house price: ${df['Price'].mean():,.2f}")
print(f"   - The price range is from ${df['Price'].min():,.2f} to ${df['Price'].max():,.2f}")

# Feature relationships
print("2. Feature Relationships:")
print(f"  - Area has the biggest impact on cost (r ≈ {num_corr.loc['Area', 'Price']:.3f})")
print("   - There is a positive correlation between price and bedrooms and bathrooms.")
print("   - YearBuilt exhibits a modest association, indicating that the cost of newer homes isn't always higher\n")

# Categorical impact
print("3. Impact of Categorical Features:")
print("   - Location has a big impact on home values. (Downtown > Suburban > Urban > Rural)")
print("   - Overall condition of the house plays a major role")
print("   - Prices for homes with garages are often higher.\n")

# Model performance
print("4. Model Evaluation:")
print(f"   - The model that performs best: {best_model_name}")
print(f"   - Highest R² Score achieved: {model_results[best_model_name]['R2 Score']:.4f}")
print(f"   - The top 3 most important characteristics {', '.join(importance_df.head(3)['Feature'].tolist())}\n")

"""#  Conclusion

##Summary

The goal was to determine what factors most affect pricing and to develop model predictions; we examined 2000 home listings for this project. Numerous machine learning methods were trained and reviewed, including Random Forest, Decision Tree, and Linear Regression. Several conclusive findings were obtained from our analysis:

## Important Findings:


1.   Top Performing Model:

    The Random Forest model showed the best performance, achieving the highest R² score of [Score Value].
    This suggests that the model explains about [Percentage]% of the variation in house prices.

2.   Factors affecting Price:

    The most important factor in determining a home's price was its size. This was followed by location, condition, and the number of bedrooms and bathrooms. Homes in better condition, with more rooms, or in desirable locations naturally have higher prices.

3.  Model Accuracy:

    The optimized Random Forest model produced a root mean square error (RMSE) of [Value]. This indicates that the predicted prices usually differ from the actual prices by about $[Value]. Therefore, the model provides reliable estimates for practical use.

### Business Consequences:

This model can be used by real estate agents to effectively advise clients and produce precise price estimates.

Homeowners are better able to figure out the factors affecting the value of their property.

By identifying properties with higher returns, investors can make more informed decisions.

## Restrictions and Upcoming Improvements:

    Predictions could be improved by additional factors like school ratings, proximity to amenities, neighborhood safety, or seasonal trends.

    Performance might be further enhanced by investigating deeper ensemble methods or neural network models.

### Overall

All things considered, this project shows how machine learning can be used to predict real estate prices. The model can be a useful tool for stakeholders in the housing market by identifying the most crucial features and offering accurate estimates.
"""

# -----------------------------
# Final Model Performance Summary
# -----------------------------

# Get best model results
best_r2 = model_results[best_model_name]['R2 Score']
best_rmse = model_results[best_model_name]['RMSE']
best_mae = model_results [best_model_name]['MAE']
top_feature = importance_df.iloc[0]['Feature']
top_feature_score = importance_df.iloc[0]['Importance']

# Print nicely formatted summary
print("### Final Model Performance Summary\n")
print(f"- Best Model: {best_model_name}")
print(f"- R² Score: {best_r2:.4f} ({best_r2*100:.2f}% of variance explained)")
print(f"- RMSE (average error): ${best_rmse:,.2f}")
print(f"- MAE (mean absolute error): ${best_mae:,.2f}")
print(f"- Most Important Feature: {top_feature} (Importance Score: {top_feature_score:.3f})")